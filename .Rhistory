install.packages("servr")
library(LDAvis)
phi <- posterior(lda)$terms
theta <- posterior(lda)$topics
vocab <- colnames(phi)
doc_length <- rowSums(as.matrix(dtm))
term_frequency <- colSums(as.matrix(dtm))
json_lda <- createJSON(
phi = phi,
theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = term_frequency
)
serVis(json_lda, open.browser = TRUE)
serVis(json_lda, out.dir = 'lda_vis', open.browser = FALSE)
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
terms_matrix <- posterior(lda)$terms
for (i in 1:k) {
wordcloud(
words = colnames(terms_matrix),
freq = terms_matrix[i, ],
min.freq = 0.001,
max.words = 50,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
main = paste("Topic", i)
)
}
for (i in 1:k) {
wordcloud(
words = colnames(terms_matrix),
freq = terms_matrix[i, ],
min.freq = 0.001,
max.words = 500,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
main = paste("Topic", i)
)
}
for (i in 1:k) {
wordcloud(
words = colnames(terms_matrix),
freq = terms_matrix[i, ],
min.freq = 0.001,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
main = paste("Topic", i)
)
}
for (i in 1:k) {
wordcloud(
words = colnames(terms_matrix),
freq = terms_matrix[i, ],
min.freq = 0.001,
max.words = 80,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
main = paste("Topic", i)
)
}
for (i in 1:k) {
wordcloud(
words = colnames(terms_matrix),
freq = terms_matrix[i, ],
min.freq = 0.001,
max.words = 70,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
main = paste("Topic", i)
)
}
top_terms <- tidy(lda, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
summarise(terms = paste(term, collapse = ", "))
print(top_terms)
doc_topics <- tidy(lda, matrix = "gamma") %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup()
data$dominant_topic <- as.integer(doc_topics$topic)
doc_topics <- tidy(lda, matrix = "gamma")
doc_dominant <- doc_topics %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup() %>%
mutate(document = as.integer(document))
data <- data %>%
mutate(document = row_number())
data_with_topics <- left_join(data, doc_dominant, by = "document")
write.csv(data_with_topics, "news_with_dominant_topics.csv", row.names = FALSE)
library(readr)
news_with_dominant_topics <- read_csv("news_with_dominant_topics.csv")
View(news_with_dominant_topics)
library(readr)
news_with_dominant_topics <- read_csv("news_with_dominant_topics.csv")
View(news_with_dominant_topics)
library(ldatuning)
library(tm)
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)
library(ggforce)
library(LDAvis)
library(wordcloud)
library(RColorBrewer)
data <- read.csv("ids_final_project_group_11_news_clean.csv", stringsAsFactors = FALSE)
corpus <- VCorpus(VectorSource(data$clear_content))
dtm <- DocumentTermMatrix(corpus, control = list(
wordLengths = c(3, Inf),
removeNumbers = TRUE,
removePunctuation = TRUE,
stopwords = TRUE
))
dtm <- removeSparseTerms(dtm, 0.98)
result <- FindTopicsNumber(
dtm,
topics = seq(2, 10, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 1234),
mc.cores = 2L,
verbose = TRUE
)
FindTopicsNumber_plot(result)
k <- result$topics[which.max(result$Griffiths2004)]
lda <- LDA(dtm, k = k, method = "Gibbs", control = list(seed = 1234))
lda_terms <- tidy(lda, matrix = "beta")
top_terms <- lda_terms %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms <- top_terms %>%
mutate(term_label = paste0(term, "_", topic))
ggplot(top_terms, aes(x = reorder(term_label, beta), y = beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_discrete(labels = ~ gsub("_\\d+$", "", .)) +
labs(title = "Top 10 Terms per Topic", x = NULL, y = "Word Probability")
phi <- posterior(lda)$terms
theta <- posterior(lda)$topics
vocab <- colnames(phi)
doc_length <- rowSums(as.matrix(dtm))
term_frequency <- colSums(as.matrix(dtm))
json_lda <- createJSON(
phi = phi,
theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = term_frequency
)
json_lda <- createJSON(
phi = phi,
theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = term_frequency
)
serVis(json_lda, open.browser = TRUE)
serVis(json_lda, out.dir = 'lda_vis', open.browser = FALSE)
terms_matrix <- posterior(lda)$terms
for (i in 1:k) {
wordcloud(
words = colnames(terms_matrix),
freq = terms_matrix[i, ],
min.freq = 0.001,
max.words = 70,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
main = paste("Topic", i)
)
}
top_terms <- tidy(lda, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
summarise(terms = paste(term, collapse = ", "))
print(top_terms)
doc_topics <- tidy(lda, matrix = "gamma")
doc_dominant <- doc_topics %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup() %>%
mutate(document = as.integer(document))
data <- data %>%
mutate(document = row_number())
data_with_topics <- left_join(data, doc_dominant, by = "document")
write.csv(data_with_topics, "news_with_dominant_topics.csv", row.names = FALSE)
library(readr)
news_with_dominant_topics <- read_csv("news_with_dominant_topics.csv")
View(news_with_dominant_topics)
gamma_matrix <- posterior(lda)$topics
gamma_df <- as.data.frame(gamma_matrix)
gamma_df$document <- 1:nrow(gamma_df)
data_with_all_topics <- data %>%
mutate(document = row_number()) %>%
left_join(gamma_df, by = "document")
print(top_terms)
doc_topics <- tidy(lda, matrix = "gamma")
doc_dominant <- doc_topics %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup() %>%
mutate(document = as.integer(document))
data <- data %>%
mutate(document = row_number())
data_with_topics <- left_join(data, doc_dominant, by = "document")
gamma_matrix <- posterior(lda)$topics
gamma_df <- as.data.frame(gamma_matrix)
gamma_df$document <- 1:nrow(gamma_df)
data_with_all_topics <- data %>%
mutate(document = row_number()) %>%
left_join(gamma_df, by = "document")
write.csv(data_with_all_topics, "news_with_all_topic_probabilities.csv", row.names = FALSE)
library(readr)
news_with_all_topic_probabilities <- read_csv("news_with_all_topic_probabilities.csv")
View(news_with_all_topic_probabilities)
library(tm)
library(tokenizers)
library(qdapRegex)
library(stringi)
library(dplyr)
library(stringr)
library(SnowballC)
library(textstem)
replace_contraction <- function(text) {
contractions <- c(
"won't" = "will not", "can't" = "cannot", "shan't" = "shall not",
"n't" = " not", "'re" = " are", "'s" = " is", "'d" = " would",
"'ll" = " will", "'t" = " not", "'ve" = " have", "'m" = " am",
# Domain-specific contractions/possessives
"Trump's" = "Trump", "Zelensky's" = "Zelensky", "Zelensky’s" = "Zelensky",
"country's" = "country", "Congo's" = "Congo", "Israel's" = "Israel",
"Nat’l" = "Nat", "int’l" = "int", "govt's" = "government",
"let's" = "let us", "ain't" = "is not", "y'all" = "you all",
"ma'am" = "madam", "o'clock" = "of the clock", "ne'er" = "never",
"gimme" = "give me", "gonna" = "going to", "gotta" = "got to",
"lemme" = "let me", "wanna" = "want to", "whatcha" = "what are you",
"could've" = "could have", "should've" = "should have",
"would've" = "would have", "might've" = "might have", "must've" = "must have",
"it's" = "it is", "that's" = "that is", "this's" = "this is",
"there's" = "there is", "here's" = "here is", "who's" = "who is",
"what's" = "what is", "when's" = "when is", "where's" = "where is",
"why's" = "why is", "how's" = "how is",
"they'd" = "they would", "they'll" = "they will", "they're" = "they are", "they've" = "they have",
"we'd" = "we would", "we're" = "we are", "we'll" = "we will", "we've" = "we have",
"i'd" = "i would", "i'll" = "i will", "i'm" = "i am", "i've" = "i have",
"he'd" = "he would", "he'll" = "he will", "he's" = "he is",
"she'd" = "she would", "she'll" = "she will", "she's" = "she is",
"you'd" = "you would", "you'll" = "you will", "you're" = "you are", "you've" = "you have",
"it'd" = "it would", "it'll" = "it will"
)
for (pattern in names(contractions)) {
text <- gsub(pattern, contractions[[pattern]], text, ignore.case = TRUE)
}
return(text)
}
normalize_politics <- function(text) {
text <- str_replace_all(text, "\\b(awamilig|al league|al\\b|awami\\b)\\b", "Awami League")
text <- str_replace_all(text, "\\b(bnp|bangladesh nationalist party)\\b", "BNP")
text <- str_replace_all(text, "\\b(jamaat|jamaat-e-islami)\\b", "Jamaat-e-Islami")
text <- str_replace_all(text, "\\b(pm|prime minister)\\b", "Prime Minister")
text <- str_replace_all(text, "\\b(mp|member of parliament)\\b", "Member of Parliament")
text <- str_replace_all(text, "\\b(ec|election commission)\\b", "Election Commission")
text <- str_replace_all(text, "\\b(govt|gov|government\\b)", "Government")
text <- str_replace_all(text, "\\b(US|USA\\b)", "United States")
return(text)
}
preprocess_text_full <- function(text) {
if (is.na(text) || nchar(text) < 2) {
return(c(tokens = "", clean_text = ""))
}
text <- normalize_politics(text)
text <- tolower(text)
text <- replace_contraction(text)
text <- gsub("\\[[^\\]]*\\]", "", text)
text <- gsub("\\([^\\)]*\\)", "", text)
text <- gsub("\\{[^\\}]*\\}", "", text)
text <- rm_url(text)
text <- removePunctuation(text)
text <- stripWhitespace(text)
text <- trimws(text)
tokens <- unlist(tokenize_words(text))
tokens <- tokens[!tokens %in% stopwords("en")]
lemmatized <- lemmatize_words(tokens)
stemmed <- wordStem(lemmatized)
return(c(
tokens = paste(tokens, collapse = "| "),
clean_text = paste(stemmed, collapse = " ")
))
}
data <- read.csv("ids_final_project_group_11_news_raw.csv", stringsAsFactors = FALSE)
content_results <- lapply(data$content, preprocess_text_full)
content_df <- as.data.frame(do.call(rbind, content_results), stringsAsFactors = FALSE)
colnames(content_df) <- c("content_tokens", "clear_content")
title_results <- lapply(data$title, preprocess_text_full)
title_df <- as.data.frame(do.call(rbind, title_results), stringsAsFactors = FALSE)
colnames(title_df) <- c("title_tokens", "clear_title")
data <- cbind(data, title_df, content_df)
data <- data %>% select(-section, -url, -title, -content)
write.csv(data, "ids_final_project_group_11_news_clean.csv", row.names = FALSE)
cat("✅ Text preprocessing complete. Cleaned file saved as: ids_final_project_group_11_news_clean.csv\n")
library(tm)
library(tokenizers)
library(qdapRegex)
library(stringi)
library(dplyr)
library(stringr)
library(SnowballC)
library(textstem)
replace_contraction <- function(text) {
contractions <- c(
"won't" = "will not", "can't" = "cannot", "shan't" = "shall not",
"n't" = " not", "'re" = " are", "'s" = " is", "'d" = " would",
"'ll" = " will", "'t" = " not", "'ve" = " have", "'m" = " am",
# Domain-specific contractions/possessives
"Trump's" = "Trump", "Zelensky's" = "Zelensky", "Zelensky’s" = "Zelensky",
"country's" = "country", "Congo's" = "Congo", "Israel's" = "Israel",
"Nat’l" = "Nat", "int’l" = "int", "govt's" = "government",
"let's" = "let us", "ain't" = "is not", "y'all" = "you all",
"ma'am" = "madam", "o'clock" = "of the clock", "ne'er" = "never",
"gimme" = "give me", "gonna" = "going to", "gotta" = "got to",
"lemme" = "let me", "wanna" = "want to", "whatcha" = "what are you",
"could've" = "could have", "should've" = "should have",
"would've" = "would have", "might've" = "might have", "must've" = "must have",
"it's" = "it is", "that's" = "that is", "this's" = "this is",
"there's" = "there is", "here's" = "here is", "who's" = "who is",
"what's" = "what is", "when's" = "when is", "where's" = "where is",
"why's" = "why is", "how's" = "how is",
"they'd" = "they would", "they'll" = "they will", "they're" = "they are", "they've" = "they have",
"we'd" = "we would", "we're" = "we are", "we'll" = "we will", "we've" = "we have",
"i'd" = "i would", "i'll" = "i will", "i'm" = "i am", "i've" = "i have",
"he'd" = "he would", "he'll" = "he will", "he's" = "he is",
"she'd" = "she would", "she'll" = "she will", "she's" = "she is",
"you'd" = "you would", "you'll" = "you will", "you're" = "you are", "you've" = "you have",
"it'd" = "it would", "it'll" = "it will"
)
for (pattern in names(contractions)) {
text <- gsub(pattern, contractions[[pattern]], text, ignore.case = TRUE)
}
return(text)
}
normalize_politics <- function(text) {
text <- str_replace_all(text, "\\b(awamilig|al league|al\\b|awami\\b)\\b", "Awami League")
text <- str_replace_all(text, "\\b(bnp|bangladesh nationalist party)\\b", "BNP")
text <- str_replace_all(text, "\\b(jamaat|jamaat-e-islami)\\b", "Jamaat-e-Islami")
text <- str_replace_all(text, "\\b(pm|prime minister)\\b", "Prime Minister")
text <- str_replace_all(text, "\\b(mp|member of parliament)\\b", "Member of Parliament")
text <- str_replace_all(text, "\\b(ec|election commission)\\b", "Election Commission")
text <- str_replace_all(text, "\\b(govt|gov|government\\b)", "Government")
text <- str_replace_all(text, "\\b(US|USA\\b)", "United States")
return(text)
}
preprocess_text_full <- function(text) {
if (is.na(text) || nchar(text) < 2) {
return(c(tokens = "", clean_text = ""))
}
text <- normalize_politics(text)
text <- tolower(text)
text <- replace_contraction(text)
text <- gsub("\\[[^\\]]*\\]", "", text)
text <- gsub("\\([^\\)]*\\)", "", text)
text <- gsub("\\{[^\\}]*\\}", "", text)
text <- rm_url(text)
text <- removePunctuation(text)
text <- stripWhitespace(text)
text <- trimws(text)
tokens <- unlist(tokenize_words(text))
tokens <- tokens[!tokens %in% stopwords("en")]
lemmatized <- lemmatize_words(tokens)
stemmed <- wordStem(lemmatized)
return(c(
tokens = paste(tokens, collapse = ", "),
clean_text = paste(stemmed, collapse = " ")
))
}
data <- read.csv("ids_final_project_group_11_news_raw.csv", stringsAsFactors = FALSE)
content_results <- lapply(data$content, preprocess_text_full)
content_df <- as.data.frame(do.call(rbind, content_results), stringsAsFactors = FALSE)
colnames(content_df) <- c("content_tokens", "clear_content")
title_results <- lapply(data$title, preprocess_text_full)
title_df <- as.data.frame(do.call(rbind, title_results), stringsAsFactors = FALSE)
colnames(title_df) <- c("title_tokens", "clear_title")
data <- cbind(data, title_df, content_df)
data <- data %>% select(-section, -url, -title, -content)
write.csv(data, "ids_final_project_group_11_news_clean.csv", row.names = FALSE)
cat("✅ Text preprocessing complete. Cleaned file saved as: ids_final_project_group_11_news_clean.csv\n")
library(ldatuning)
library(tm)
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)
library(ggforce)
library(LDAvis)
library(wordcloud)
library(RColorBrewer)
data <- read.csv("ids_final_project_group_11_news_clean.csv", stringsAsFactors = FALSE)
corpus <- VCorpus(VectorSource(data$clear_content))
dtm <- DocumentTermMatrix(corpus, control = list(
wordLengths = c(3, Inf),
removeNumbers = TRUE,
removePunctuation = TRUE,
stopwords = TRUE
))
dtm <- removeSparseTerms(dtm, 0.98)
result <- FindTopicsNumber(
dtm,
topics = seq(2, 10, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 1234),
mc.cores = 2L,
verbose = TRUE
)
FindTopicsNumber_plot(result)
k <- result$topics[which.max(result$Griffiths2004)]
lda <- LDA(dtm, k = k, method = "Gibbs", control = list(seed = 1234))
lda_terms <- tidy(lda, matrix = "beta")
top_terms <- lda_terms %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms <- top_terms %>%
mutate(term_label = paste0(term, "_", topic))
ggplot(top_terms, aes(x = reorder(term_label, beta), y = beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_discrete(labels = ~ gsub("_\\d+$", "", .)) +
labs(title = "Top 10 Terms per Topic", x = NULL, y = "Word Probability")
phi <- posterior(lda)$terms
theta <- posterior(lda)$topics
vocab <- colnames(phi)
doc_length <- rowSums(as.matrix(dtm))
term_frequency <- colSums(as.matrix(dtm))
json_lda <- createJSON(
phi = phi,
theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = term_frequency
)
json_lda <- createJSON(
phi = phi,
theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = term_frequency
)
serVis(json_lda, open.browser = TRUE)
serVis(json_lda, out.dir = 'lda_vis', open.browser = FALSE)
terms_matrix <- posterior(lda)$terms
for (i in 1:k) {
wordcloud(
words = colnames(terms_matrix),
freq = terms_matrix[i, ],
min.freq = 0.001,
max.words = 70,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
main = paste("Topic", i)
)
}
top_terms <- tidy(lda, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
summarise(terms = paste(term, collapse = ", "))
print(top_terms)
doc_topics <- tidy(lda, matrix = "gamma")
doc_dominant <- doc_topics %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup() %>%
mutate(document = as.integer(document))
data <- data %>%
mutate(document = row_number())
data_with_topics <- left_join(data, doc_dominant, by = "document")
gamma_matrix <- posterior(lda)$topics
gamma_df <- as.data.frame(gamma_matrix)
gamma_df$document <- 1:nrow(gamma_df)
data_with_all_topics <- data %>%
mutate(document = row_number()) %>%
left_join(gamma_df, by = "document")
write.csv(data_with_all_topics, "news_with_all_topic_probabilities.csv", row.names = FALSE)
gamma_long <- gamma_df %>%
pivot_longer(cols = -document, names_to = "topic", values_to = "probability") %>%
mutate(topic = as.integer(gsub("V", "", topic)))
library(tidyr)
gamma_long <- gamma_df %>%
pivot_longer(cols = -document, names_to = "topic", values_to = "probability") %>%
mutate(topic = as.integer(gsub("V", "", topic)))
top_docs_per_topic <- gamma_long %>%
left_join(data, by = c("document")) %>%
group_by(topic) %>%
arrange(desc(probability)) %>%
slice_head(n = 5) %>%  # Top 5 per topic
ungroup()
write.csv(top_docs_per_topic, "top_documents_per_topic.csv", row.names = FALSE)
library(readr)
top_documents_per_topic <- read_csv("top_documents_per_topic.csv")
View(top_documents_per_topic)
library(readr)
news_with_dominant_topics <- read_csv("news_with_dominant_topics.csv")
View(news_with_dominant_topics)
